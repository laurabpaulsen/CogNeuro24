{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "In this notebook we will prepare the preprocessed data for analysis by creating epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python\n",
    "First of all, we need to make sure that we are working in the `env` environment.\n",
    "\n",
    "\n",
    "1. Run `bash env_to_jupyter.sh` from the `EEG` folder if you have not already done so. This will make sure that the `env` environment is available as a kernel in this notebook.\n",
    "\n",
    "2. Press `Select Kernel`, then `Jupyter kernel...` and select `env`. If `env` does not show up, press the little refresh symbol!\n",
    "\n",
    "**Note:** You might have to install the Jupyter extension for VScode to be able to select the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the epochs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(os.getcwd(), 'epochs')\n",
    "filename = \"epochs-epo.fif\"\n",
    "\n",
    "epochs = mne.read_epochs(os.path.join(input_path, filename), preload=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP analysis\n",
    "Now that the data is epoched and resampled, we can start to analyse the data. \n",
    "\n",
    "**Group the epochs by modality (i.e. visual and auditory)**\n",
    "\n",
    "*Hint: See this [link](https://mne.tools/stable/auto_tutorials/epochs/10_epochs_overview.html#subselecting-epochs) on how to subselect epochs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs = epochs['visual']\n",
    "auditory_epochs = epochs['auditory']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use the plot_image() method of the Epochs class to plot the channel 'EEG 021' for both auditory and visual epochs**\n",
    "\n",
    "These plots show each epoch as one row of the image map for the chosen channel, with color representing signal magnitude. The average evoked response and the sensor location are shown below the image. This gives us an idea of whether there is consistency in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs.plot_image(picks=['EEG 021']);\n",
    "auditory_epochs.plot_image(picks=['EEG 021']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "**Q:** In your view, which is the most consistent signal of the two? Why do you think that is? (*hints: where is the EEG 021 channel located on the scalp? Try using plot_sensors and flagging show_names=True*). **A:**\n",
    "\n",
    "**Q:** Which channel would you choose if you were to reverse the pattern of the signal consistencies? Why? Try it out! **A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sensor locations\n",
    "epochs.plot_sensors(show_names=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_epochs.plot_image(picks=['EEG 059']);\n",
    "auditory_epochs.plot_image(picks=['EEG 059']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evoked responses\n",
    "Unlike `Epochs`, which contain multiple trials that are each associated with a condition label (that is the event ID), `Evoked` objects are averages across trials for a single condition. Thus we have to create a separate `Evoked` object for each condition in our experiment.\n",
    "\n",
    "**Create an Evoked object for each modality**\n",
    "\n",
    "*Hint: use the average() method of the Epochs class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_visual = visual_epochs.average()\n",
    "evoked_auditory = auditory_epochs.average()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the two evokeds**\n",
    "\n",
    "*Hint: use mne.viz.plot_compare_evokeds()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds({\"auditory\":evoked_auditory, \"visual\":evoked_visual})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a closer look at these averaged responses, here visualised with the global field power (GFP). GFP is actually the same as taking the standard deviation across all channels in each time sample. This tells us in which timepoints we see the biggest difference between different channels, which in turn is indicative of some kind of (more or less) focal activity. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "**Q:** Looking at the plot, where in time do is the standard deviation across all channels the highest for visual and auditory, respectively? **A:**\n",
    "\n",
    "\n",
    "**Q:** Do those timepoints match any well-known components relevant in this context? (*hint: N100 and N170*) **A:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets use the plot_joint() method of the Evoked class to plot the evoked responses for both modalities.**\n",
    "\n",
    "This function generates a combination of “butterfly” plots (waveforms of all electrodes overlaid) and topographic maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_auditory.plot_joint();\n",
    "evoked_visual.plot_joint();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "**Q:** Does these plots match your observations from the GFP-plots? **A:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra exercises\n",
    "\n",
    "If you haven't had a go at artefact removal using ICA, now is the time to go back to the preprocessing notebook and try it out! Another option is to complete the extra exercises here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1E. Contrasting the responses of visual stimuli presented to the left and right visual field\n",
    "In the previous exercises we have contrasted the response from visual and auditory stimuli. Now we will contrast the responses from visual stimuli presented to the left and right visual field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: create vis_left_epochs and vis_right_epochs\n",
    "vis_left_epochs = epochs[\"visual/left\"]\n",
    "vis_right_epochs = epochs[\"visual/right\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: create evoked_left and evoked_right\n",
    "evoked_left = vis_left_epochs.average()\n",
    "evoked_right = vis_right_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: compare evoked responses for left vs. right visual stimuli\n",
    "mne.viz.plot_compare_evokeds({\"left\":evoked_left, \"right\":evoked_right})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe with a few sentences what the plot shows!**\n",
    "\n",
    "**Now lets have a look at the topography of the difference between the two conditions.** \n",
    "\n",
    "Hint: use the plot_topomap() method of the Evoked class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_left.plot_topomap(times = np.linspace(0.0, 0.2, 5));\n",
    "evoked_right.plot_topomap(times = np.linspace(0.0, 0.2, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe the difference in topography between the two conditions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2E. Contrasting the responses of auditory stimuli presented to the left and right ear\n",
    "Do the same thing for auditory stimuli presented to the left and right ear!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
