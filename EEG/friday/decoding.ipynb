{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding analysis\n",
    "This notebook demonstrates a machine learning approach to analyse the data. The goal is to predict the stimulus from the neural activity. This information based approach is different from the traditional hypothesis testing approach. In the hypothesis testing approach, we test whether the neural activity is significantly different between different two conditions. In the information based approach, we test whether the neural activity contains information about the condition.\n",
    "\n",
    "\n",
    "If you are interested in reading some neuroscience papers that uses decoding analysis, here are some examples:\n",
    "- [Decoding Brain Responses to Names and Voices across Different Vigilance States](https://www.mdpi.com/1424-8220/21/10/3393)\n",
    "- [Resolving human object recognition in space and time](https://www.nature.com/articles/nn.3635)\n",
    "- [Neural dynamics of phoneme sequences reveal position-invariant code for content and order](https://www.nature.com/articles/s41467-022-34326-1)\n",
    "\n",
    "More methods oriented papers:\n",
    "- [Characterizing the dynamics of mental representations: the temporal generalization method](https://www.sciencedirect.com/science/article/abs/pii/S1364661314000199)\n",
    "- [Information-based functional brain mapping](https://www.pnas.org/doi/10.1073/pnas.0600244103) (mainly for fMRI, but the idea is the same)\n",
    "- [Deconstructing multivariate decoding for the study of brain function](https://pubmed.ncbi.nlm.nih.gov/28782682/)\n",
    "\n",
    "and at last, a technical review paper discussing interpretations both of activation and information based analysis. More specifically they argue that most analysis focus on the information we can measure as experimenters (experimenter-as-receiver), rather than on how the rest of the brain interpret those activations (cortex-as-receiver). Can be helpful in terms of interpreting and discussing results:\n",
    "- [Does neuroimaging measure information in the brain?](https://pubmed.ncbi.nlm.nih.gov/26833316/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python\n",
    "First of all, we need to make sure that we are working in the `env` environment.\n",
    "\n",
    "\n",
    "1. Run `bash env_to_ipynb_kernel.sh` from the `EEG` folder if you have not already done so. This will make sure that the `env` environment is available as a kernel in this notebook.\n",
    "\n",
    "2. Press `Select Kernel`, then `Jupyter kernel...` and select `env`. If `env` does not show up, press the little refresh symbol!\n",
    "\n",
    "**Note:** You might have to install the Jupyter extension for VScode to be able to select the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from one participant\n",
    "data_path = Path(\"/work/EEG_lab/example_data\")\n",
    "epochs = mne.read_epochs(data_path / \"Group1-epo.fif\", verbose=False, preload=True)\n",
    "\n",
    "# only keep eeg channels\n",
    "epochs = epochs.pick([\"eeg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding using data from a single subject\n",
    "This notebook focuses on within-subject decoding. That is we train a model on data from one subject and test it on the same subject. However, we should always use a separate dataset for training and testing to avoid overfitting. This means that we cannot train and test the model on data from the same trials from our subject. Therefore, we use k-fold cross-validation. If you are not familiar with cross-validation, you can read more about it [here](https://spotintelligence.com/2023/07/29/k-fold-cross-validation/). \n",
    "\n",
    "\n",
    "Some studies also conduct cross-subject decoding, where the model is trained on one subject and tested on another subject. This is useful for testing whether the model can generalize to new subjects - however this is not introduced in this notebook, but should be relatively easy to implement relying on the code in this notebook. \n",
    "\n",
    "For the purpose of this tutorial, we will see if we can decode whether the subject is presented with a word or an image (i.e., smiley). Therefore, we begin by extracting the data from the two conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the word from the first participant\n",
    "words = epochs[\"Word\"].get_data()\n",
    "\n",
    "# get all the image from the first participant (smiley face)\n",
    "images = epochs[\"Image\"].get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a variable that contains the labels for the two conditions *y*. We will use 0 for the word condition and 1 for the image condition. Furthermore we concatenate the data from the two conditions into one variable *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0] * len(words) + [1] * len(images)\n",
    "y = np.array(y)\n",
    "\n",
    "# concatenate the data along the trials axis\n",
    "X = np.concatenate([words, images], axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the data and the labels, we can start the decoding analysis. We will use the `scikit-learn` library for this purpose. This library contains a lot of useful tools for machine learning.\n",
    "\n",
    "We will be using a linear discriminant analysis (LDA) classifier. LDA is a simple and fast classifier that is often used for decoding analysis. If you are not familiar with LDA, you can read more about it [here](https://scikit-learn.org/stable/modules/lda_qda.html).\n",
    "\n",
    "First we will be using a simple decoding analysis, where we train the model on data from a single time point. Then we will use a more advanced decoding analysis, where we test the decoding accuracy at all time points. Finally, we will use a temporal generalization analysis, where we train the model on data from one time point and test it on all time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding at one time point\n",
    "Lets begin by focusing on one time point. We will use the first time point, but you can change this to any time point you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from timesample 1\n",
    "X_t1 = X[:, :, 1]\n",
    "\n",
    "print(X_t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not want to train and test our model on the same data, we will split the data into some cross-validation folds. We will use 5 folds, so we will train the model on 4/5 of the data and test it on the remaining 1/5 of the data. We will repeat this 5 times, so that we have tested the model on all the data. For the purpose of splitting the data, I have defined a function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_splits(X, y, n_splits=5, shuffle=True, balance=True, stratify=True):\n",
    "    \"\"\"\n",
    "    Function to create cross validation splits\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        The data to split of shape (n_trials, n_channels) or (n_trials, n_channels, n_timesamples)\n",
    "\n",
    "    y : np.ndarray\n",
    "        The labels of shape (n_trials,) with 0 indicating the first class and 1 the second class\n",
    "\n",
    "    n_splits : int\n",
    "        The number of splits to create\n",
    "    \n",
    "    shuffle : bool\n",
    "        Whether to shuffle the data before splitting\n",
    "    \n",
    "    balance : bool\n",
    "        Whether to balance the data before splitting (i.e. have the same number of trials for each class in each split)\n",
    "\n",
    "    stratify : bool\n",
    "        Whether to stratify the data before splitting (i.e. have the same class distribution in each split)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Xs : list\n",
    "        A list of length n_splits containing the data splits\n",
    "    \n",
    "    ys : list\n",
    "        A list of length n_splits containing the label splits\n",
    "    \"\"\"\n",
    "\n",
    "    if balance:\n",
    "        # Get the indices of the trials for each class\n",
    "        idx_0 = np.where(y == 0)[0]\n",
    "        idx_1 = np.where(y == 1)[0]\n",
    "\n",
    "        # Get the minimum number of trials for each class\n",
    "        min_trials = min(len(idx_0), len(idx_1))\n",
    "\n",
    "        # Randomly sample the indices and concatenate them\n",
    "        idx = np.concatenate([np.random.choice(idx_0, min_trials, replace=False),\n",
    "                              np.random.choice(idx_1, min_trials, replace=False)])\n",
    "\n",
    "        # Get the data and labels for the balanced data\n",
    "        X, y = X[idx], y[idx]\n",
    "\n",
    "    idx = np.arange(len(X))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    if stratify:\n",
    "        # Get the indices of the trials for each class\n",
    "        idx_0 = np.where(y == 0)[0]\n",
    "        idx_1 = np.where(y == 1)[0]\n",
    "\n",
    "        # Concatenate the indices for each class\n",
    "        idx_splits = [np.concatenate([np.array_split(idx_class, n_splits)[i] for idx_class in [idx_0, idx_1]]) for i in range(n_splits)]\n",
    "\n",
    "        # Randomly shuffle the indices for each split\n",
    "        idx_splits = [np.random.permutation(split) for split in idx_splits]\n",
    "\n",
    "        # Concatenate the indices\n",
    "        idx = np.concatenate(idx_splits)\n",
    "\n",
    "    # split the data into n_splits\n",
    "    splits = np.array_split(idx, n_splits)\n",
    "\n",
    "    Xs = [X[split] for split in splits]\n",
    "    ys = [y[split] for split in splits]\n",
    "\n",
    "    return Xs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5 # number of splits to create (5 is arbitrary, you can change this. The more splits, the more accurate the estimate of the accuracy but the longer it takes to run)\n",
    "\n",
    "# create cross validation splits\n",
    "Xs, ys = cross_val_splits(\n",
    "    X_t1, \n",
    "    np.array(y), \n",
    "    n_splits = n_splits,\n",
    "    shuffle = True,\n",
    "    balance = True,\n",
    "    stratify = True\n",
    ")\n",
    "\n",
    "# make the classifier object\n",
    "clf = lda()\n",
    "\n",
    "accuracies = np.array([])\n",
    "\n",
    "# loop over the splits\n",
    "for i in range(n_splits):\n",
    "    # get the train and test data\n",
    "    X_train = np.concatenate([Xs[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    X_test = Xs[i]\n",
    "\n",
    "    y_train = np.concatenate([ys[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    y_test = ys[i]\n",
    "\n",
    "    # fit the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # get the accuracy\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    # append the accuracy to the list\n",
    "    accuracies = np.append(accuracies, accuracy)\n",
    "\n",
    "print(f\"accuracies for each split: {accuracies}\")\n",
    "print(f\"mean accuracy: {np.mean(accuracies)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding at all time points\n",
    "Now we have tested the ability to decode the condition at one time point. However, it might be interesting to see how the decoding accuracy changes over the duration of the trial. We can do this by looping through all the time points and decoding the condition at each time point. We will store the decoding accuracy for each time point in a variable *accuracy*.\n",
    "\n",
    "This allows us to plot the decoding accuracy as a function of time, which informs when the condition can be decoded from the neural activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cross validation splits\n",
    "Xs, ys = cross_val_splits(\n",
    "    X, \n",
    "    np.array(y), \n",
    "    n_splits = n_splits,\n",
    "    shuffle = True,\n",
    "    balance = True,\n",
    "    stratify = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a shape (n_splits, n_timesamples) array to store the accuracies\n",
    "accuracies = np.zeros((n_splits, X.shape[2]))\n",
    "\n",
    "# make a shape to store the beta coefficients (n_splits, n_channels, n_timesamples)\n",
    "betas = np.zeros((n_splits, X.shape[1], X.shape[2]))\n",
    "\n",
    "# prepare the scaler for scaling the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# loop over the splits\n",
    "for i in range(n_splits):\n",
    "    # get the train and test data\n",
    "    X_train = np.concatenate([Xs[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    X_test = Xs[i]\n",
    "\n",
    "    y_train = np.concatenate([ys[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    y_test = ys[i]\n",
    "\n",
    "    # loop over the timesamples\n",
    "    for t in range(X_train.shape[2]):\n",
    "        \n",
    "        # scale the data (notice that we fit the scaler to the training data and not the test data)\n",
    "        X_train_scaled = scaler.fit_transform(X_train[:, :, t])\n",
    "        X_test_scaled = scaler.transform(X_test[:, :, t])\n",
    "\n",
    "        # fit the classifier\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # get the accuracy\n",
    "        accuracy = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "        # save the beta coefficients\n",
    "        betas[i, :, t] = clf.coef_\n",
    "\n",
    "        # save the accuracy\n",
    "        accuracies[i, t] = accuracy\n",
    "\n",
    "print(f\"mean accuracy: {np.mean(accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the decoding accuracy over time\n",
    "Now we can plot the decoding accuracy as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy over time\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "# take the mean over the splits\n",
    "accuracies_avg = np.mean(accuracies, axis=0)\n",
    "\n",
    "# times instead of timesamples (downsampled to 250 hz in preprocessing and epoched from -200 ms to 500 ms)\n",
    "times = np.arange(-200, 500, 1000/250)\n",
    "\n",
    "# plot the accuracies\n",
    "ax.plot(times, accuracies_avg)\n",
    "\n",
    "# plot a horizontal line at chance level (0.5) BEAWARE THAT WE HAVE FEW TRIALS, SO CHANCE LEVEL CAN EXEEED 0.5 BY CHANCE\n",
    "ax.axhline(0.5, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# plot a vertical line at 0 ms indicating onset\n",
    "ax.axvline(0, color=\"black\", linestyle=\"solid\", linewidth=1)\n",
    "\n",
    "ax.set_title(\"Decoding accuracy of word vs image\")\n",
    "\n",
    "ax.set_xlabel(\"Time relative to onset (ms)\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of beta (weights) over time\n",
    "We can also plot the weights of the model as a function of time. This can give us an idea of which electrodes are important for the decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing some times to plot\n",
    "times_to_plot = np.arange(0, len(times), 10)\n",
    "\n",
    "fig, axes = plt.subplots(3, len(times_to_plot)//3,  dpi=300, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    mne.viz.plot_topomap(betas[:, :, times_to_plot[i]].mean(axis=0), epochs.info, axes=ax, show=False, contours=0)\n",
    "    ax.set_title(f\"{times[times_to_plot[i]]} ms\")\n",
    "\n",
    "# plot \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal generalisation\n",
    "So far we have only looked at decoding the condition at each time point. However, we can also use the model trained at one time point to predict the condition at another time point. This is called temporal generalisation. We can do this by looping through all the time points and training the model at one time point and testing it at all other time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a shape (n_splits, n_timesamples, n_timesamples) array to store the accuracies\n",
    "accuracies = np.zeros((n_splits, X.shape[2], X.shape[2]))\n",
    "\n",
    "# prepare the scaler for scaling the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# loop over the splits\n",
    "for i in range(n_splits):\n",
    "    # get the train and test data\n",
    "    X_train = np.concatenate([Xs[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    X_test = Xs[i]\n",
    "\n",
    "    y_train = np.concatenate([ys[j] for j in range(n_splits) if j != i], axis=0)\n",
    "    y_test = ys[i]\n",
    "\n",
    "    # loop over the timesamples for training\n",
    "    for t in range(X_train.shape[2]):\n",
    "        print(f\"split {i+1}/{n_splits}, time {t+1}/{X_train.shape[2]}\")\n",
    "        \n",
    "        # scale the data (notice that we fit the scaler to the training data and not the test data)\n",
    "        X_train_scaled = scaler.fit_transform(X_train[:, :, t])\n",
    "\n",
    "        # fit the classifier\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # loop over the timesamples for testing\n",
    "        for tt in range(X_test.shape[2]):\n",
    "            X_test_scaled = scaler.transform(X_test[:, :, tt])\n",
    "\n",
    "\n",
    "            # get the accuracy\n",
    "            accuracy = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "            # save the accuracy\n",
    "            accuracies[i, t, tt] = accuracy\n",
    "\n",
    "print(f\"mean accuracy: {np.mean(accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the temporal generalisation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean over the splits\n",
    "accuracies_avg = np.mean(accuracies, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10), dpi=300)\n",
    "\n",
    "# plot the accuracies\n",
    "im = ax.imshow(accuracies_avg, vmin=0.2, vmax=0.8, origin=\"lower\", cmap=\"RdBu_r\", interpolation=\"gaussian\")\n",
    "\n",
    "# times instead of timesamples (downsampled to 250 hz in preprocessing and epoched from -200 ms to 500 ms) on the x and y axis\n",
    "times = np.arange(-200, 500, 1000/250)\n",
    "times = times.astype(int)\n",
    "\n",
    "# only plot every 20th timepoint\n",
    "ax.set_xticks(np.arange(len(times))[::20])\n",
    "ax.set_xticklabels(times[::20])\n",
    "ax.set_yticks(np.arange(len(times))[::20])\n",
    "ax.set_yticklabels(times[::20])\n",
    "\n",
    "# label the axes\n",
    "ax.set_xlabel(\"Time relative to onset (ms)\")\n",
    "ax.set_ylabel(\"Time relative to onset (ms)\")\n",
    "\n",
    "ax.set_title(\"Temporal generalisation of word vs image\")\n",
    "\n",
    "# add a colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.5)\n",
    "cbar.set_label(\"Accuracy\")\n",
    "\n",
    "# plot lines at 0 indicating onset\n",
    "ax.axhline(50, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.axvline(50, color=\"black\", linestyle=\"--\", linewidth=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
